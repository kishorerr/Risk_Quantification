{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"module3.csv\",encoding='latin-1')\n",
    "df_pay = df[\"injection_type\"]\n",
    "df.drop([\"payload\"],axis=1,inplace=True)\n",
    "df.drop([\"injection_type\"],axis=1,inplace=True)\n",
    "X = df.values\n",
    "y = df_pay.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train , X_test , y_train , y_test = train_test_split(X ,y , test_size=0.2,random_state= 2) \n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "y_test = y_test.type(torch.LongTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BayesianNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BayesianNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item()\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    return test_loss / len(test_loader), correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 9\n",
    "hidden_size = 1024\n",
    "output_size = 4\n",
    "num_epochs = 15\n",
    "batch_size = 512\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.8358, Test Loss: 0.1014, Test Accuracy: 0.9740\n",
      "Epoch 2 - Train Loss: 0.1594, Test Loss: 0.1111, Test Accuracy: 0.9744\n",
      "Epoch 3 - Train Loss: 0.1168, Test Loss: 0.0920, Test Accuracy: 0.9746\n",
      "Epoch 4 - Train Loss: 0.1062, Test Loss: 0.0739, Test Accuracy: 0.9782\n",
      "Epoch 5 - Train Loss: 0.0765, Test Loss: 0.0750, Test Accuracy: 0.9779\n",
      "Epoch 6 - Train Loss: 0.0774, Test Loss: 0.0797, Test Accuracy: 0.9764\n",
      "Epoch 7 - Train Loss: 0.0772, Test Loss: 0.1170, Test Accuracy: 0.9649\n",
      "Epoch 8 - Train Loss: 0.0762, Test Loss: 0.0821, Test Accuracy: 0.9780\n",
      "Epoch 9 - Train Loss: 0.0689, Test Loss: 0.0808, Test Accuracy: 0.9781\n",
      "Epoch 10 - Train Loss: 0.0700, Test Loss: 0.0776, Test Accuracy: 0.9768\n",
      "Epoch 11 - Train Loss: 0.0704, Test Loss: 0.0764, Test Accuracy: 0.9763\n",
      "Epoch 12 - Train Loss: 0.0686, Test Loss: 0.0720, Test Accuracy: 0.9794\n",
      "Epoch 13 - Train Loss: 0.0716, Test Loss: 0.0729, Test Accuracy: 0.9782\n",
      "Epoch 14 - Train Loss: 0.0655, Test Loss: 0.0720, Test Accuracy: 0.9771\n",
      "Epoch 15 - Train Loss: 0.0670, Test Loss: 0.0716, Test Accuracy: 0.9766\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, optimizer, criterion, train_loader)\n",
    "    test_loss, test_accuracy = evaluate(model, criterion, test_loader)\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:/Users/kisho/Documents/GitHub/Risk_Quantification/Threat_Identification\"\n",
    "\n",
    "def dump(model , filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        joblib.dump(model , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dump(model , filename\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m/bnn.pkl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "dump(model , filename=f\"{file_path}/bnn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('/Users/kisho/Documents/GitHub/Risk_Quantification')\n",
    "sys.path\n",
    "\n",
    "from WAF_Model_Trainer.utils import extract_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([25,  0,  5,  5,  0,  0,  0,  0,  4], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_n = \"&& perl -e 'print \"\"X\"\"x4096'\"\n",
    "d = extract_feature(payload_n)\n",
    "d.drop(columns=d.columns[0], axis=1,  inplace=True)\n",
    "im_arr = np.array(d.values)\n",
    "im_arr = im_arr.flatten()\n",
    "im_arr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND INJECTION\n"
     ]
    }
   ],
   "source": [
    "new_data = torch.Tensor(im_arr)\n",
    "bnn_model = joblib.load(\"bnn.pkl\")\n",
    "\n",
    "predicted_class = predict(bnn_model, new_data.unsqueeze(0))\n",
    "if(predicted_class.item() == 0) :\n",
    "    attack_type = \"NORMAL\"\n",
    "elif(predicted_class.item() == 1) :\n",
    "    attack_type = \"COMMAND INJECTION\"\n",
    "elif(predicted_class.item() == 2) :\n",
    "    attack_type = \"SQL INJECTION\"\n",
    "else:\n",
    "    attack_type = \"CROSS-SITE SCRIPTING (XSS)\"\n",
    "\n",
    "print(attack_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attacks = pd.read_csv(\"C:/Users/kisho/Documents/GitHub/Risk_Quantification/Logged_Output/waf_payloads.csv\")\n",
    "fin_arr = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_attacks.iterrows():\n",
    "    current_row = []\n",
    "    current_payload = row['Malicious Payload']\n",
    "    current_row.append(current_payload)\n",
    "    d = extract_feature(current_payload)\n",
    "    d.drop(columns=d.columns[0], axis=1,  inplace=True)\n",
    "    im_arr = np.array(d.values)\n",
    "    im_arr = im_arr.flatten()\n",
    "    current_data = torch.Tensor(im_arr)\n",
    "    predicted_class = predict(bnn_model, current_data.unsqueeze(0))\n",
    "    if(predicted_class.item() == 0) :\n",
    "        attack_type = \"NORMAL\"\n",
    "    elif(predicted_class.item() == 1) :\n",
    "        attack_type = \"COMMAND INJECTION\"\n",
    "    elif(predicted_class.item() == 2) :\n",
    "        attack_type = \"SQL INJECTION\"\n",
    "    else:\n",
    "        attack_type = \"CROSS-SITE SCRIPTING (XSS)\"\n",
    "    current_row.append(attack_type)\n",
    "    fin_arr.append(current_row)\n",
    "    \n",
    "fin_arr = np.array(fin_arr)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Payload': fin_arr[:, 0], 'Attack Type': fin_arr[:, 1]})\n",
    "df.info\n",
    "df.to_csv(\"C:/Users/kisho/Documents/GitHub/Risk_Quantification/Logged_Output/bnn_attack_types.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "317abd80653085121c5a0f1070ab2e662fefca692dc3af3b1a569244903cfcf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
